{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predictive_Typing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvHZAPxa-EBQ"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import Pool\n",
        "from typing import List, Dict, Tuple, Callable, Iterable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter, defaultdict\n",
        "import regex as re\n",
        "import nltk\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BOS = '<BOS>'\n",
        "EOS = '<EOS>'"
      ],
      "metadata": {
        "id": "xKoBz4fWHP6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrieNode:\n",
        "    \n",
        "    def __init__(self, char: str) -> None:\n",
        "        self.char: str = char\n",
        "        self.is_end: bool = False\n",
        "        self.count: int = 0\n",
        "        self.children: Dict = {}\n",
        "\n",
        "class Trie:\n",
        "\n",
        "    def __init__(self, words: Iterable[str] = None):\n",
        "        self.root = TrieNode(\"\")\n",
        "        \n",
        "        if words:\n",
        "            for word in words:\n",
        "                self.insert(word)\n",
        "        \n",
        "    def insert(self, word: str) -> None:\n",
        "        \n",
        "        node: TrieNode = self.root\n",
        "        \n",
        "        for char in word:\n",
        "            if char in node.children:\n",
        "                node = node.children[char]\n",
        "            else:\n",
        "                new_node: TrieNode = TrieNode(char)\n",
        "                node.children[char] = new_node\n",
        "                node = new_node\n",
        "        node.is_end = True\n",
        "        node.count += 1\n",
        "        \n",
        "    def dfs(self, node, prefix):\n",
        "        \n",
        "        if node.is_end:\n",
        "            self.output.append((prefix + node.char, node.count))\n",
        "            \n",
        "        for child in node.children.values():\n",
        "            self.dfs(child, prefix + node.char)\n",
        "    \n",
        "    def query(self, prefix: str) -> List[Tuple[str, int]]:\n",
        "        \n",
        "        self.output = []\n",
        "        node = self.root\n",
        "        \n",
        "        for char in prefix:\n",
        "            if char in node.children:\n",
        "                node = node.children[char]\n",
        "            else:\n",
        "                return []\n",
        "            \n",
        "        self.dfs(node, prefix[:-1])\n",
        "        \n",
        "        return sorted(self.output, key=lambda x: -x[1])\n"
      ],
      "metadata": {
        "id": "nmCQ6csrT2sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment if you are using colab"
      ],
      "metadata": {
        "id": "jV81cAdkGGDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./data\n",
        "!wget https://raw.githubusercontent.com/vadim0912/MLIntro2021/main/lecture08/data/train.csv.zip -O ./data/train.csv.zip\n",
        "!wget https://raw.githubusercontent.com/vadim0912/MLIntro2021/main/lecture08/data/test.csv.zip -O ./data/test.csv.zip"
      ],
      "metadata": {
        "id": "Yun89AYgFbN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"./data/train.csv.zip\")\n",
        "test_df = pd.read_csv(\"./data/test.csv.zip\")\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "PWCwjAD6GD4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrams_count(token_text, n_ngrams):\n",
        "\n",
        "    counts = defaultdict(Counter)\n",
        "    for tokens in token_text:\n",
        "        for ngram in nltk.ngrams(tokens, n=n_ngrams, pad_left=True, left_pad_symbol=BOS, pad_right=True, right_pad_symbol=EOS):\n",
        "            counts[' '.join(ngram[:-1])][ngram[-1]] += 1\n",
        "\n",
        "    return counts"
      ],
      "metadata": {
        "id": "iTCWr_MYfsbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordPredict:\n",
        "    \n",
        "    def __init__(self, tokens, n_ngrams):\n",
        "        \n",
        "        self.n_ngrams = n_ngrams\n",
        "        self.tokens = tokens\n",
        "    \n",
        "    def compute_count(self):\n",
        "\n",
        "        for prev, dist in ngrams_count(self.tokens, self.n_ngrams).items():\n",
        "            self.proba[prev] = Counter({\n",
        "                token : count / sum(dist.values()) for token, count in dist.items()})"
      ],
      "metadata": {
        "id": "qC5IR4aFGKC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_bad_sym(text):\n",
        "    return re.sub('[0-9^\\p,.\\-?!–«»\"\":+]', ' ', text)\n",
        "\n",
        "def get_token(text):\n",
        "    return nltk.wordpunct_tokenize(text)"
      ],
      "metadata": {
        "id": "s-5_qAgVIDjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.sentence = train_df.sentence.apply(lambda x : remove_bad_sym(x))\n",
        "df_sentence = train_df.sentence.apply(lambda x : get_token(x))"
      ],
      "metadata": {
        "id": "zbQIflkOIv8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = df_sentence.values.tolist()"
      ],
      "metadata": {
        "id": "UYPdqnuJKc9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trie_predict(sentence: str, trie: Trie) -> str:\n",
        "\n",
        "    model = WordPredict(tokens, 2)\n",
        "    model.compute_count()\n",
        "\n",
        "    what_pred = sentence.split()[-1]\n",
        "    pred = model.proba[sentence.split()[-2:-1][0]]\n",
        "\n",
        "    if pred:\n",
        "\n",
        "        max_prob = 0\n",
        "        pred_word = ''\n",
        "\n",
        "        for word, prob in pred.items():\n",
        "            if word.startswith(what_pred) and max_prob < prob:\n",
        "                max_prob = prob\n",
        "                pred_word = word\n",
        "\n",
        "        if max_prob != 0:\n",
        "            return pred_word\n",
        "            \n",
        "    pred_trie = trie.query(what_pred)\n",
        "\n",
        "    return pred_trie[0][0] if pred_trie else what_pred\n",
        "\n",
        "def pd_func(df) -> pd.DataFrame:\n",
        "    df['token'] = df['prefix'].apply(lambda x: trie_predict(x, trie))\n",
        "    return df\n",
        "\n",
        "def parallelize_dataframe(df: pd.DataFrame, func: Callable, n_cores: int) -> pd.DataFrame:\n",
        "    with Pool(n_cores) as pool:\n",
        "        results = pool.map(func, np.array_split(df, n_cores))\n",
        "    return pd.concat(results)"
      ],
      "metadata": {
        "id": "gMOlUFHYUO_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trie = Trie(\n",
        "     word for sentence in tokens for word in sentence\n",
        ")"
      ],
      "metadata": {
        "id": "XOkvhbXmJs4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = parallelize_dataframe(test_df, pd_func, n_cores=4)"
      ],
      "metadata": {
        "id": "vGXoMKvaWlTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "id": "94lPLF4-_zk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred[['index', 'token']].to_csv(\"simple_baseline.csv\", index=False)"
      ],
      "metadata": {
        "id": "qHmtjejuWwZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}